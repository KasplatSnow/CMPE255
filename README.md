# CMPE 255 â€” Keystroke Dynamics User Classification

This project performs biometric user identification based on keystroke dynamics from the [DSL-StrongPassword dataset](https://www.cs.cmu.edu/~keystroke/).

We use models like **XGBoost**, **Random Forest**, and **MLP** to classify users by their typing patterns on a fixed password.

---

## ğŸ“¦ Project Structure

```
â”œâ”€â”€ data/
â”‚   â””â”€â”€ DSL-StrongPasswordData.csv       # Raw dataset
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ final_results.ipynb              # Visualizations & evaluation
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ metrics.csv                      # Accuracy/F1/AUC logs
â”‚   â””â”€â”€ figures/
â”‚       â”œâ”€â”€ confmat_xgb.png              # Confusion matrix plot
â”‚       â””â”€â”€ confmat_rf.png
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py                         # Main training script
â”‚   â”œâ”€â”€ models.py                        # Model factory
â”‚   â”œâ”€â”€ preprocessing.py                 # Data pipeline (clipping, scaling, filtering)
â”‚   â””â”€â”€ utils.py                         # Logging, seeding, metric saving
â””â”€â”€ README.md
```

---

## ğŸš€ Setup Instructions

```bash
python -m venv venv
source venv/bin/activate       # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

---

## ğŸ” Run the Pipeline

Train on the top-10 most frequent users using XGBoost:

```bash
python src/train.py --model xgb
```

Other model choices: `rf`, `logreg`, `mlp`, `knn`

---

## ğŸ“Š Outputs

- `results/metrics.csv` â€“ accuracy, f1, and AUC per model run
- `results/figures/confmat_<model>.png` â€“ confusion matrix
- Sample metric:
  ```
  {'model': 'xgb', 'acc': 0.932, 'f1': 0.932, 'auc': 0.9965}
  ```

---

## âš™ï¸ Preprocessing

Each repetition of the password is converted into 91 features:
- 31 key hold times (`H.`)
- 30 digraph down-down latencies (`DD.`)
- 30 up-down transitions (`UD.`)

We apply:
- Outlier clipping at the 99th percentile
- Standard scaling
- Low variance filtering (threshold = 0.0005)

---

## ğŸ§  Model Summary

| Model       | Notes                                          |
|-------------|-------------------------------------------------|
| MLP         | Best performer (93.6% accuracy, best F1 & AUC)  |
| XGBoost     | Strong generalization (93.2% accuracy)          |
| KNN         | Competitive baseline (91.7% accuracy)           |
| RandomForest| Fast, interpretable, slightly lower AUC         |
| LogReg      | Simple linear baseline                          |

---

## ğŸ“š Acknowledgments

Dataset: [CMU DSL Strong Password Dataset](https://www.cs.cmu.edu/~keystroke/)
